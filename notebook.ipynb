{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility code to convert from CSV to feather for faster load times"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import feather\n",
    "\n",
    "#################### READ CSV #########################\n",
    "# df = pd.read_csv('./all/train.csv', parse_dates=['pickup_datetime'])\n",
    "################## WRITE FEATHER ######################\n",
    "# df.to_feather('./all/train.feather')\n",
    "################## READ FEATHER #######################\n",
    "# df = pd.read_feather('./all/train.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe the original dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantly recognizable outliers:\n",
    "* **passenger_count**\n",
    "    * Passenger counts of 0\n",
    "    * Infeasible passenger counts (e.g. 200). Taxis can legally hold up to 4-6 passengers (http://www.nyc.gov/html/tlc/html/faq/faq_pass.shtml)\n",
    "* **fare_amount**\n",
    "    * Negative fare amounts\n",
    "    * Extremely high fare amounts (thousands of dollars)\n",
    "* **coordinates**\n",
    "    * Invalid coordinates (out of defined range)\n",
    "    * Coordinates outside of NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Data cleaning functions:\n",
    "#### Check for invalid coordinates (outside of NYC range), determined with https://www.mapdevelopers.com/geocode_bounding_box.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_coordinates(lat_list, lon_list):\n",
    "    for i in lat_list:\n",
    "        if i < 40.477399 or i > 40.917577:\n",
    "            return False\n",
    "    for i in lon_list:\n",
    "        if i < -74.259090 or i > -73.700272:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def clean_coordinates(df):\n",
    "    df.query('~(pickup_latitude < 40.477399 or pickup_latitude > 40.917577) &\\\n",
    "              ~(dropoff_latitude < 40.477399 or dropoff_latitude > 40.917577) &\\\n",
    "              ~(pickup_longitude < -74.259090 or pickup_longitude > -73.700272) &\\\n",
    "              ~(dropoff_longitude < -74.259090 or dropoff_longitude > -73.700272)', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove invalid passenger counts, fares. Upper bound to both. Also drop missing values and key column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pfdk(df):\n",
    "    df.drop(columns=['key'], inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df.query('passenger_count > 0 &\\\n",
    "              passenger_count <= 6 &\\\n",
    "              fare_amount > 0 &\\\n",
    "              fare_amount <= 100 ', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Feature Engineering functions:\n",
    "***\n",
    "#### Calculate Euclidean Distance (and drop rows with distance = 0 if training). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "    return ((x2-x1)**2 + (y2-y1)**2)**0.5\n",
    "\n",
    "def add_euclidean_distance(df, training=True):\n",
    "    df['euclidean_distance'] = euclidean_distance(df['pickup_latitude'], \n",
    "                                                  df['pickup_longitude'], \n",
    "                                                  df['dropoff_latitude'], \n",
    "                                                  df['dropoff_longitude'])\n",
    "    if training:\n",
    "        df.query('euclidean_distance > 0', inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Manhattan Distance (and drop rows with distance = 0 if training). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(x1, y1, x2, y2):\n",
    "    return abs(x2-x1) + abs(y2-y1)\n",
    "\n",
    "def add_manhattan_distance(df, training=True):\n",
    "    df['manhattan_distance'] = manhattan_distance(df['pickup_latitude'], \n",
    "                                                  df['pickup_longitude'], \n",
    "                                                  df['dropoff_latitude'], \n",
    "                                                  df['dropoff_longitude'])\n",
    "    if training:\n",
    "        df.query('manhattan_distance > 0', inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because of fixed fares, also calculate distance (euclidean) to/from airports.\n",
    "* Coordinates determined with a Google Maps query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_jfk(df):\n",
    "    df['go_jfk'] = euclidean_distance(df['dropoff_latitude'], df['dropoff_longitude'], 40.6413, -73.7781)\n",
    "    df['leave_jfk'] = euclidean_distance(df['pickup_latitude'], df['pickup_longitude'], 40.6413, -73.7781)\n",
    "def add_lga(df):\n",
    "    df['go_lga'] = euclidean_distance(df['dropoff_latitude'], df['dropoff_longitude'], 40.7769, -73.8740)\n",
    "    df['leave_lga'] = euclidean_distance(df['pickup_latitude'], df['pickup_longitude'], 40.7769, -73.8740)\n",
    "def add_ewr(df):\n",
    "    df['go_ewr'] = euclidean_distance(df['dropoff_latitude'], df['dropoff_longitude'], 40.6895, -74.1745)\n",
    "    df['leave_ewr'] = euclidean_distance(df['pickup_latitude'], df['pickup_longitude'], 40.6895, -74.1745)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Time Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_times(df):\n",
    "    df['year']  = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day']   = df['pickup_datetime'].dt.day\n",
    "    df['hour']  = df['pickup_datetime'].dt.hour + (df['pickup_datetime'].dt.minute/60)\n",
    "    df.drop(columns=['pickup_datetime'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External data: Monthly Gas Prices acquired from https://data.bls.gov/timeseries/APU000074714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gas_prices(df):\n",
    "    gas_df = pd.read_csv('./all/gas_prices.csv')\n",
    "    gas_dict = {}\n",
    "    for row in gas_df.itertuples():\n",
    "        year = row[1]\n",
    "        for i in range(2, len(row)):\n",
    "            gas_dict['{0}-{1}'.format(year, i-1)] = row[i]\n",
    "    df['year-month']  = df['year'].astype(str) + '-' + df['month'].astype(str)\n",
    "    df['gas'] = df['year-month'].map(gas_dict)\n",
    "    df.drop(columns=['year-month'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, training=True):\n",
    "    if training:\n",
    "        clean_pfdk(df)\n",
    "        clean_coordinates(df)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    add_euclidean_distance(df, training)\n",
    "    add_manhattan_distance(df, training)\n",
    "    add_times(df)\n",
    "    add_gas_prices(df)\n",
    "    add_jfk(df)\n",
    "    add_lga(df)\n",
    "    add_ewr(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility code to preprocess training data and save as feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feather\n",
    "\n",
    "############## PREPROCESS TRAINING DATA $##############\n",
    "# preprocess(df, training=True)\n",
    "############## WRITE PREPROCESSED FEATHER #############\n",
    "# df.to_feather('./all/preprocessed_train.feather')\n",
    "######### READ PREPROCESSED FEATHER ###################\n",
    "df = pd.read_feather('./all/preprocessed_train.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Visualization:\n",
    "#### Scatter plot (Euclidean Distance, Fare Amount)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "corr = df['euclidean_distance'].corr(df['fare_amount'])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(df['euclidean_distance'], df['fare_amount'], alpha=0.2)\n",
    "plt.title('ScatterPlot (Euclidean Distance, Fare Amount)\\nCorrelation = {0}'.format(corr), fontsize=20)\n",
    "plt.xlabel('Euclidean Distance', fontsize=18)\n",
    "plt.ylabel('Fare Amount', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Very strong correlation (0.87)\n",
    "* There are three groups of fixed fares over any distance. This makes sense because there are usually fixed fares to JFK, Newark, and LaGuardia airports. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ScatterPlot (Time of Day, Distance Traveled)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corr = df['hour'].corr(df['euclidean_distance'])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(df['hour'], df['euclidean_distance'], alpha=0.05)\n",
    "plt.title('ScatterPlot (Time of Day, Distance Traveled)\\nCorrelation = {0}'.format(corr), fontsize=20)\n",
    "plt.xlabel('Time of day', fontsize=18)\n",
    "plt.ylabel('Euclidean distance', fontsize=18)\n",
    "plt.xlim(0,24)\n",
    "plt.xticks(range(0,25))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Seems to be very low/negligible correlation (-0.03)\n",
    "* Naturally, less people travel in the early morning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ScatterPlot (Time of Day, Fare Amount)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corr = df['hour'].corr(df['fare_amount'])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(df['hour'], df['fare_amount'], alpha=0.05)\n",
    "plt.title('ScatterPlot (Time of Day, Fare Amount)\\nCorrelation = {0}'.format(corr), fontsize=20)\n",
    "plt.xlabel('Time of day', fontsize=18)\n",
    "plt.ylabel('Fare Amount', fontsize=18)\n",
    "plt.xlim(0,24)\n",
    "plt.xticks(range(0,25))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Correlation also very weak (-0.018)\n",
    "* Three groups of fixed fares also visible here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Heatmap"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(corr_matrix, cmap='BuGn', annot=True, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Euclidean distance has the highest correlation with fare amount\n",
    "* Manhattan distance is almost perfectly correlated with euclidean\n",
    "* Distance to/from airports have significant correlation with fare\n",
    "* Latidude is negatively correlated with fare\n",
    "* Longitude is positively correlated with fare\n",
    "* Year has a correlation of 0.12 with fare. Makes sense since costs slowly rise with inflation.\n",
    "* Gas prices and year have significant correlation (0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fares by Destination"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(df['dropoff_longitude'], df['dropoff_latitude'], c=df['fare_amount'], cmap=plt.get_cmap('jet'), alpha=0.05)\n",
    "colorbar = plt.colorbar()\n",
    "colorbar.ax.set_ylabel('Fare Amount', fontsize=20)\n",
    "plt.title('Fares by Destination'.format(corr), fontsize = 20)\n",
    "plt.xlabel('Longitude', fontsize=18)\n",
    "plt.ylabel('Latitude', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***   \n",
    "# Train/Test split wrapper function\n",
    "Can specify number of rows to train on and test subset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT TRAIN/TEST DATA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_tt_split(df, test_size=0.2):\n",
    "    y = df['fare_amount']\n",
    "    X = df.drop(columns=['fare_amount', 'manhattan_distance'])\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Model: Linear Regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_lr(df):\n",
    "    X_train, X_test, y_train, y_test = get_tt_split(df, test_size=0.005)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    print('Training model: {0}...'.format('lr'))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    test_predictions = model.predict(X_test)\n",
    "    print('Mean Squared Error (Training):', mean_squared_error(y_test, test_predictions))\n",
    "    print('\\nCOEFFICIENTS:')\n",
    "    for i in list(zip(X_test.columns, model.coef_)):\n",
    "        print('{0}: {1}'.format(i[0], i[1]))\n",
    "    print('==============================')\n",
    "    print('\\nINTERCEPT:', model.intercept_)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Results:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "train_lr(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Most important feature:\n",
    "* Scaling has no effect on performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Decision Tree Ensembles (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST (DECISION TREE ENSEMBLES)\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import DMatrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_xgb(df):\n",
    "    X_train, X_test, y_train, y_test = get_tt_split(df, test_size=0.005)\n",
    "    \n",
    "    print('Training model: {0}...'.format('xgb'))\n",
    "    model = xgb.train(params={'eta':0.6}, dtrain=DMatrix(X_train, y_train))\n",
    "    test_predictions = model.predict(xgb.DMatrix(X_test))\n",
    "    print('Mean Squared Error (Training):', mean_squared_error(y_test, test_predictions))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: xgb...\n",
      "Mean Squared Error (Training): 10.894632028215339\n",
      "Wall time: 10min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0x526f887c18>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_xgb(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURAL NETWORK\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.train import RMSPropOptimizer\n",
    "from tensorflow.nn import relu, softmax\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def train_nn(df):\n",
    "    X_train, X_test, y_train, y_test = get_tt_split(df.head(50000), test_size=0.2)\n",
    "    X_train = MinMaxScaler().fit_transform(X_train)\n",
    "    X_test = MinMaxScaler().fit_transform(X_test)\n",
    "    \n",
    "    shape = (len(X_train[0]),)\n",
    "    learning_rate = 0.01\n",
    "    model = keras.Sequential([Dense(128, input_shape=shape), Dense(128, activation=relu), Dense(1)])\n",
    "    model.compile(loss='mse', metrics=['mse'], optimizer=RMSPropOptimizer(learning_rate))\n",
    "    print('Training model: {0}...'.format('nn'))\n",
    "    model.fit(X_train, y_train, epochs=300, verbose=0)\n",
    "\n",
    "    test_predictions = [i[0] for i in model.predict(X_test)]\n",
    "    print('Mean Squared Error (Training):', mean_squared_error(y_test, test_predictions))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: nn...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_nn(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Outperforms Linear Regression model with only a small fraction of the data\n",
    "* Takes significantly longer than other models to run\n",
    "* Scaling improves performance significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 'xgb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess/fit model to real test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Predictions written to submissions.csv\n"
     ]
    }
   ],
   "source": [
    "real_df = pd.read_csv('./all/test.csv', parse_dates=['pickup_datetime'])\n",
    "preprocess(real_df, training=False)\n",
    "\n",
    "real_X = real_df.drop(columns=['key', 'manhattan_distance'])\n",
    "\n",
    "if selected_model == 'lr':\n",
    "    model = train_lr(df)\n",
    "    predictions = model.predict(real_X)\n",
    "elif selected_model == 'xgb':\n",
    "    model = train_xgb(df)\n",
    "    predictions = model.predict(xgb.DMatrix(real_X))\n",
    "elif selected_model == 'nn':\n",
    "    model = train_nn(df)\n",
    "    real_X = MinMaxScaler().fit_transform(real_X)\n",
    "    predictions = [i[0] for i in model.predict(real_X)]\n",
    "\n",
    "data = list(zip(real_df['key'], predictions))\n",
    "\n",
    "submission = pd.DataFrame(data, columns=['key', 'fare_amount'])\n",
    "submission.set_index('key', inplace=True)\n",
    "submission.to_csv('submission.csv')\n",
    "print('----------------------------')\n",
    "print('Predictions written to submissions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Kaggle Scores by model (RMSE):\n",
    "* Linear Regression: 5.64822\n",
    "* Neural Network: 4.36043\n",
    "* Decision Tree Ensembles (XGBoost): 3.36031   <---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
