{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility code to convert from CSV to feather for faster load times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feather\n",
    "\n",
    "#################### READ CSV #########################\n",
    "# df = pd.read_csv('./all/train.csv', parse_dates=['pickup_datetime'])\n",
    "################## WRITE FEATHER ######################\n",
    "# df.to_feather('./all/train.feather')\n",
    "################## READ FEATHER #######################\n",
    "df = pd.read_feather('./all/train.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantly recognizable outliers:\n",
    "* **passenger_count**\n",
    "    * Passenger counts of 0\n",
    "    * Infeasible passenger counts (e.g. 200). Taxis can legally hold up to 4-6 passengers (http://www.nyc.gov/html/tlc/html/faq/faq_pass.shtml)\n",
    "* **fare_amount**\n",
    "    * Negative fare amounts\n",
    "    * Extremely high fare amounts (thousands of dollars)\n",
    "* **coordinates**\n",
    "    * Invalid coordinates (out of defined range)\n",
    "    * Coordinates outside of NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Data cleaning functions:\n",
    "#### Check for invalid coordinates (outside of NYC range), determined with https://www.mapdevelopers.com/geocode_bounding_box.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_coordinates(lat_list, lon_list):\n",
    "    for i in lat_list:\n",
    "        if i < 40.477399 or i > 40.917577:\n",
    "            return False\n",
    "    for i in lon_list:\n",
    "        if i < -74.259090 or i > -73.700272:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def clean_coordinates(df):\n",
    "    df.query('~(pickup_latitude < 40.477399 or pickup_latitude > 40.917577) &\\\n",
    "              ~(dropoff_latitude < 40.477399 or dropoff_latitude > 40.917577) &\\\n",
    "              ~(pickup_longitude < -74.259090 or pickup_longitude > -73.700272) &\\\n",
    "              ~(dropoff_longitude < -74.259090 or dropoff_longitude > -73.700272)', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove invalid passenger counts, fares. Upper bound to both. Also drop missing values and key column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pfdk(df):\n",
    "    df.drop(columns=['key'], inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df.query('passenger_count > 0 &\\\n",
    "              passenger_count <= 6 &\\\n",
    "              fare_amount > 0 &\\\n",
    "              fare_amount <= 100 ', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Feature Engineering functions:\n",
    "***\n",
    "#### Calculate Distance (and drop rows with distance = 0 if training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "    return ((x2-x1)**2 + (y2-y1)**2)**0.5\n",
    "\n",
    "def manhattan_distance(x1, y1, x2, y2):\n",
    "    return abs(x2-x1) + abs(y2-y1)\n",
    "    \n",
    "def add_euclidean_distance(df, training=True):\n",
    "    df['euclidean_distance'] = euclidean_distance(df['pickup_latitude'], df['pickup_longitude'], df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "    if training:\n",
    "        df.query('euclidean_distance > 0', inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "def add_manhattan_distance(df, training=True):\n",
    "    df['manhattan_distance'] = manhattan_distance(df['pickup_latitude'], df['pickup_longitude'], df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "    if training:\n",
    "        df.query('manhattan_distance > 0', inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Time Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_times(df):\n",
    "    df['year']  = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day']   = df['pickup_datetime'].dt.day\n",
    "    df['hour']  = df['pickup_datetime'].dt.hour + (df['pickup_datetime'].dt.minute/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External data: Monthly Gas Prices acquired from https://data.bls.gov/timeseries/APU000074714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gas_prices(df):\n",
    "    gas_df = pd.read_csv('./all/gas_prices.csv')\n",
    "    gas_dict = {}\n",
    "    for row in gas_df.itertuples():\n",
    "        year = row[1]\n",
    "        for i in range(2, len(row)):\n",
    "            gas_dict['{0}-{1}'.format(year, i-1)] = row[i]\n",
    "    df['year-month']  = df['year'].astype(str) + '-' + df['month'].astype(str)\n",
    "    df['gas'] = df['year-month'].map(gas_dict)\n",
    "    df.drop(columns=['year-month'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, training=True):\n",
    "    if training:\n",
    "        clean_pfdk(df)\n",
    "        clean_coordinates(df)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    add_euclidean_distance(df, training)\n",
    "    add_manhattan_distance(df, training)\n",
    "    add_times(df)\n",
    "    add_gas_prices(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility code to preprocess training data and save as feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feather\n",
    "\n",
    "############## PREPROCESS TRAINING DATA $##############\n",
    "#preprocess(df, training=True)\n",
    "############## WRITE PREPROCESSED FEATHER #############\n",
    "#df.to_feather('./all/preprocessed_train.feather')\n",
    "######### READ PREPROCESSED FEATHER ###################\n",
    "df = pd.read_feather('./all/preprocessed_train.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Visualization:\n",
    "#### Scatter plot (Euclidean Distance, Fare Amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "corr = df['euclidean_distance'].corr(df['fare_amount'])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(df['euclidean_distance'], df['fare_amount'], alpha=0.2)\n",
    "plt.title('ScatterPlot (Euclidean Distance, Fare Amount)\\nCorrelation = {0}'.format(corr), fontsize = 20)\n",
    "plt.xlabel('Euclidean Distance', fontsize=18)\n",
    "plt.ylabel('Fare Amount', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Very strong correlation (0.87)\n",
    "* There are three groups of fixed fares over any distance. This makes sense because there are usually fixed fares to JFK, Newark, and LaGuardia airports. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ScatterPlot (Time of Day, Distance Traveled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df['hour'].corr(df['euclidean_distance'])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(df['hour'], df['euclidean_distance'], alpha=0.05)\n",
    "plt.title('ScatterPlot (Time of Day, Distance Traveled)\\nCorrelation = {0}'.format(corr), fontsize = 20)\n",
    "plt.xlabel('Time of day', fontsize=18)\n",
    "plt.ylabel('Euclidean distance', fontsize=18)\n",
    "plt.xlim(0,24)\n",
    "plt.xticks(range(0,25))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Seems to be very low/negligible correlation (-0.03)\n",
    "* Naturally, less people travel in the early morning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ScatterPlot (Time of Day, Fare Amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df['hour'].corr(df['fare_amount'])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(df['hour'], df['fare_amount'], alpha=0.05)\n",
    "plt.title('ScatterPlot (Time of Day, Fare Amount)\\nCorrelation = {0}'.format(corr), fontsize = 20)\n",
    "plt.xlabel('Time of day', fontsize=18)\n",
    "plt.ylabel('Fare Amount', fontsize=18)\n",
    "plt.xlim(0,24)\n",
    "plt.xticks(range(0,25))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Correlation also very weak (-0.018)\n",
    "* Three groups of fixed fares also visible here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(corr_matrix, cmap='BuGn', annot=True, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Euclidean distance has the highest correlation with fare amount\n",
    "* Latidude is negatively correlated with fare\n",
    "* Longitude is positively correlated with fare\n",
    "* Year has a correlation of 0.12 with fare. Makes sense since costs slowly rise with inflation.\n",
    "* Gas prices and year have significant correlation (0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fares by Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(df['dropoff_longitude'], df['dropoff_latitude'], c=df['fare_amount'], cmap=plt.get_cmap('jet'), alpha=0.05)\n",
    "colorbar = plt.colorbar()\n",
    "colorbar.ax.set_ylabel('Fare Amount', fontsize=20)\n",
    "plt.title('Fares by Destination'.format(corr), fontsize = 20)\n",
    "plt.xlabel('Longitude', fontsize=18)\n",
    "plt.ylabel('Latitude', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***   \n",
    "# Train/Test split wrapper function\n",
    "Can specify number of rows to train on, test subset size, and whether or not to scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT TRAIN/TEST DATA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def get_tt_split(rows, test_size=0.005, scale_X=False):\n",
    "    X = df.head(rows)[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'year', 'month', 'day', 'hour', 'euclidean_distance', 'passenger_count', 'gas']]\n",
    "    y = df.head(rows)['fare_amount']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=3)\n",
    "    \n",
    "    if scale_X:\n",
    "        X_train = StandardScaler().fit_transform(X_train)\n",
    "        X_test  = StandardScaler().fit_transform(X_test)\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Model: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_lr(df):\n",
    "    model = LinearRegression()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = get_tt_split(len(df), scale_X=False)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    test_predictions = model.predict(X_test)\n",
    "    print('Mean Squared Error (Training):', mean_squared_error(y_test, test_predictions))\n",
    "    print('\\nCOEFFICIENTS:')\n",
    "    for i in list(zip(X_test.columns, model.coef_)):\n",
    "        print('{0}: {1}'.format(i[0], i[1]))\n",
    "    print('==============================')\n",
    "    print('\\nINTERCEPT:', model.intercept_)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* most important feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Decision Tree Ensembles (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST (DECISION TREE ENSEMBLES)\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import DMatrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_xgb(df):\n",
    "    X_train, X_test, y_train, y_test = get_tt_split(len(df), scale_X=False)\n",
    "    model = xgb.train(params={}, dtrain=DMatrix(X_train, y_train))\n",
    "\n",
    "    test_predictions = model.predict(xgb.DMatrix(X_test))\n",
    "    print('Mean Squared Error (Training):', mean_squared_error(y_test, test_predictions))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURAL NETWORK\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.train import RMSPropOptimizer\n",
    "from tensorflow.nn import relu, softmax\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_nn(df):\n",
    "    X_train, X_test, y_train, y_test = get_tt_split(1000, scale_X=False)\n",
    "\n",
    "    shape = (len(X_train.columns),)\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    model = keras.Sequential([Dense(32, input_shape=shape), Dense(32, activation=relu), Dense(32, activation=relu), Dense(1)])\n",
    "    model.compile(loss='mse', metrics=['mse'], optimizer=RMSPropOptimizer(learning_rate))\n",
    "    model.fit(X_train, y_train, epochs=1000, verbose=0)\n",
    "\n",
    "    test_predictions = [i[0] for i in model.predict(X_test)]\n",
    "    print('Mean Squared Error (Training):', mean_squared_error(y_test, test_predictions))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 'xgb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess/fit model to real test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df = pd.read_csv('./all/test.csv', parse_dates=['pickup_datetime'])\n",
    "preprocess(real_df, training=False)\n",
    "\n",
    "real_X = real_df[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'year', 'month', 'day', 'hour', 'euclidean_distance', 'passenger_count', 'gas']]\n",
    "\n",
    "#if scaled:\n",
    "#    real_X = StandardScaler().fit_transform(real_X)\n",
    "\n",
    "print('Training model: {0}...'.format(selected_model))\n",
    "if selected_model == 'lr':\n",
    "    model = train_lr(df)\n",
    "    predictions = model.predict(real_X)\n",
    "elif selected_model == 'xgb':\n",
    "    model = train_xgb(df)\n",
    "    predictions = model.predict(xgb.DMatrix(real_X))\n",
    "elif selected_model == 'nn':\n",
    "    model = train_nn(df)\n",
    "    predictions = [i[0] for i in model.predict(real_X)]\n",
    "\n",
    "data = list(zip(real_df['key'], predictions))\n",
    "\n",
    "submission = pd.DataFrame(data, columns=['key', 'fare_amount'])\n",
    "submission.set_index('key', inplace=True)\n",
    "submission.to_csv('submission.csv')\n",
    "print('----------------------------')\n",
    "print('Predictions written to submissions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Scores by model (RMSE):\n",
    "* Linear Regression:\n",
    "* Neural Network:\n",
    "* Decision Tree Ensembles (XGBoost):    <---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
