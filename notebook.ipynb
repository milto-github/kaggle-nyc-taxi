{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility code to convert from CSV to feather for faster load times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feather\n",
    "\n",
    "#################### READ CSV #########################\n",
    "# df = pd.read_csv('./all/train.csv', parse_dates=['pickup_datetime'])\n",
    "################## WRITE FEATHER ######################\n",
    "# df.to_feather('./all/train.feather')\n",
    "################## READ FEATHER #######################\n",
    "df = pd.read_feather('./all/train.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantly recognizable outliers:\n",
    "* **passenger_count**\n",
    "    * Passenger counts of 0\n",
    "    * Infeasible passenger counts (e.g. 200). Taxis can legally hold up to 4-6 passengers (http://www.nyc.gov/html/tlc/html/faq/faq_pass.shtml)\n",
    "* **fare_amount**\n",
    "    * Negative fare amounts\n",
    "    * Extremely high fare amounts (thousands of dollars)\n",
    "* **coordinates**\n",
    "    * Invalid coordinates (out of defined range)\n",
    "    * Coordinates outside of NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Data cleaning functions:\n",
    "#### Check for invalid coordinates (outside of NYC range), determined with https://www.mapdevelopers.com/geocode_bounding_box.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_coordinates(lat_list, lon_list):\n",
    "    for i in lat_list:\n",
    "        if i < 40.477399 or i > 40.917577:\n",
    "            return False\n",
    "    for i in lon_list:\n",
    "        if i < -74.259090 or i > -73.700272:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def clean_coordinates(df):\n",
    "    df.query('~(pickup_latitude < 40.477399 or pickup_latitude > 40.917577) &\\\n",
    "              ~(dropoff_latitude < 40.477399 or dropoff_latitude > 40.917577) &\\\n",
    "              ~(pickup_longitude < -74.259090 or pickup_longitude > -73.700272) &\\\n",
    "              ~(dropoff_longitude < -74.259090 or dropoff_longitude > -73.700272)', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove invalid passenger counts, fares. Upper bound to both. Also drop missing values and key column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pfdk(df):\n",
    "    df.drop(columns=['key'], inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df.query('passenger_count > 0 &\\\n",
    "              passenger_count <= 6 &\\\n",
    "              fare_amount > 0 &\\\n",
    "              fare_amount <= 100 ', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Feature Engineering functions:\n",
    "***\n",
    "#### Calculate Distance (and drop rows with distance = 0 if training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "    return ((x2-x1)**2 + (y2-y1)**2)**0.5\n",
    "\n",
    "def manhattan_distance(x1, y1, x2, y2):\n",
    "    return abs(x2-x1) + abs(y2-y1)\n",
    "    \n",
    "def add_euclidean_distance(df, training=True):\n",
    "    df['euclidean_distance'] = euclidean_distance(df['pickup_latitude'], df['pickup_longitude'], df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "    if training:\n",
    "        df.query('euclidean_distance > 0', inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "def add_manhattan_distance(df, training=True):\n",
    "    df['manhattan_distance'] = manhattan_distance(df['pickup_latitude'], df['pickup_longitude'], df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "    if training:\n",
    "        df.query('manhattan_distance > 0', inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Time Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_times(df):\n",
    "    df['year']  = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day']   = df['pickup_datetime'].dt.day\n",
    "    df['hour']  = df['pickup_datetime'].dt.hour + (df['pickup_datetime'].dt.minute/60)\n",
    "    df.drop(columns=['pickup_datetime'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External data: Monthly Gas Prices acquired from https://data.bls.gov/timeseries/APU000074714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gas_prices(df):\n",
    "    gas_df = pd.read_csv('./all/gas_prices.csv')\n",
    "    gas_dict = {}\n",
    "    for row in gas_df.itertuples():\n",
    "        year = row[1]\n",
    "        for i in range(2, len(row)):\n",
    "            gas_dict['{0}-{1}'.format(year, i-1)] = row[i]\n",
    "    df['year-month']  = df['year'].astype(str) + '-' + df['month'].astype(str)\n",
    "    df['gas'] = df['year-month'].map(gas_dict)\n",
    "    df.drop(columns=['year-month'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, training=True):\n",
    "    if training:\n",
    "        clean_pfdk(df)\n",
    "        clean_coordinates(df)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    add_euclidean_distance(df, training)\n",
    "    add_manhattan_distance(df, training)\n",
    "    add_times(df)\n",
    "    add_gas_prices(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility code to preprocess training data and save as feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feather\n",
    "\n",
    "############## PREPROCESS TRAINING DATA $##############\n",
    "# preprocess(df, training=True)\n",
    "############## WRITE PREPROCESSED FEATHER #############\n",
    "# df.to_feather('./all/preprocessed_train.feather')\n",
    "######### READ PREPROCESSED FEATHER ###################\n",
    "df = pd.read_feather('./all/preprocessed_train.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Visualization:\n",
    "#### Scatter plot (Euclidean Distance, Fare Amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "corr = df['euclidean_distance'].corr(df['fare_amount'])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(df['euclidean_distance'], df['fare_amount'], alpha=0.2)\n",
    "plt.title('ScatterPlot (Euclidean Distance, Fare Amount)\\nCorrelation = {0}'.format(corr), fontsize=20)\n",
    "plt.xlabel('Euclidean Distance', fontsize=18)\n",
    "plt.ylabel('Fare Amount', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Very strong correlation (0.87)\n",
    "* There are three groups of fixed fares over any distance. This makes sense because there are usually fixed fares to JFK, Newark, and LaGuardia airports. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ScatterPlot (Time of Day, Distance Traveled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df['hour'].corr(df['euclidean_distance'])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(df['hour'], df['euclidean_distance'], alpha=0.05)\n",
    "plt.title('ScatterPlot (Time of Day, Distance Traveled)\\nCorrelation = {0}'.format(corr), fontsize=20)\n",
    "plt.xlabel('Time of day', fontsize=18)\n",
    "plt.ylabel('Euclidean distance', fontsize=18)\n",
    "plt.xlim(0,24)\n",
    "plt.xticks(range(0,25))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Seems to be very low/negligible correlation (-0.03)\n",
    "* Naturally, less people travel in the early morning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ScatterPlot (Time of Day, Fare Amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df['hour'].corr(df['fare_amount'])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(df['hour'], df['fare_amount'], alpha=0.05)\n",
    "plt.title('ScatterPlot (Time of Day, Fare Amount)\\nCorrelation = {0}'.format(corr), fontsize=20)\n",
    "plt.xlabel('Time of day', fontsize=18)\n",
    "plt.ylabel('Fare Amount', fontsize=18)\n",
    "plt.xlim(0,24)\n",
    "plt.xticks(range(0,25))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Correlation also very weak (-0.018)\n",
    "* Three groups of fixed fares also visible here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(corr_matrix, cmap='BuGn', annot=True, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Euclidean distance has the highest correlation with fare amount\n",
    "* Latidude is negatively correlated with fare\n",
    "* Longitude is positively correlated with fare\n",
    "* Year has a correlation of 0.12 with fare. Makes sense since costs slowly rise with inflation.\n",
    "* Gas prices and year have significant correlation (0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fares by Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(df['dropoff_longitude'], df['dropoff_latitude'], c=df['fare_amount'], cmap=plt.get_cmap('jet'), alpha=0.05)\n",
    "colorbar = plt.colorbar()\n",
    "colorbar.ax.set_ylabel('Fare Amount', fontsize=20)\n",
    "plt.title('Fares by Destination'.format(corr), fontsize = 20)\n",
    "plt.xlabel('Longitude', fontsize=18)\n",
    "plt.ylabel('Latitude', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***   \n",
    "# Train/Test split wrapper function\n",
    "Can specify number of rows to train on and test subset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT TRAIN/TEST DATA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_tt_split(df, nrows, test_size=0.2):\n",
    "    X = df.head(nrows)[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'year', 'month', 'day', 'hour', 'manhattan_distance', 'euclidean_distance', 'passenger_count', 'gas']]\n",
    "    y = df.head(nrows)['fare_amount']\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Model: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_lr(df):\n",
    "    X_train, X_test, y_train, y_test = get_tt_split(df, nrows=len(df), test_size=0.005)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    print('Training model: {0}...'.format('lr'))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    test_predictions = model.predict(X_test)\n",
    "    print('Mean Squared Error (Training):', mean_squared_error(y_test, test_predictions))\n",
    "    print('\\nCOEFFICIENTS:')\n",
    "    for i in list(zip(X_test.columns, model.coef_)):\n",
    "        print('{0}: {1}'.format(i[0], i[1]))\n",
    "    print('==============================')\n",
    "    print('\\nINTERCEPT:', model.intercept_)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: lr...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-3afda8d5d1dc>\u001b[0m in \u001b[0;36mtrain_lr\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training model: {0}...'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtest_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    487\u001b[0m         X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n\u001b[0;32m    488\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             copy=self.copy_X, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36m_preprocess_data\u001b[1;34m(X, y, fit_intercept, normalize, copy, sample_weight, return_mean)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'],\n\u001b[1;32m--> 168\u001b[1;33m                     dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m    169\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    431\u001b[0m                                       force_all_finite)\n\u001b[0;32m    432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_lr(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Most important feature:\n",
    "* Scaling has no effect on performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Decision Tree Ensembles (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST (DECISION TREE ENSEMBLES)\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import DMatrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_xgb(df):\n",
    "    X_train, X_test, y_train, y_test = get_tt_split(df, nrows=len(df), test_size=0.005)\n",
    "    \n",
    "    print('Training model: {0}...'.format('lr'))\n",
    "    model = xgb.train(params={'eta':0.6}, dtrain=DMatrix(X_train, y_train))\n",
    "\n",
    "    test_predictions = model.predict(xgb.DMatrix(X_test))\n",
    "    print('Mean Squared Error (Training):', mean_squared_error(y_test, test_predictions))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_xgb(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURAL NETWORK\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.train import RMSPropOptimizer\n",
    "from tensorflow.nn import relu, softmax\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def train_nn(df):\n",
    "    X_train, X_test, y_train, y_test = get_tt_split(df, nrows=100000, test_size=0.2)\n",
    "    X_train = MinMaxScaler().fit_transform(X_train)\n",
    "    X_test = MinMaxScaler().fit_transform(X_test)\n",
    "    \n",
    "    shape = (len(X_train[0]),)\n",
    "    learning_rate = 0.01\n",
    "    model = keras.Sequential([Dense(128, input_shape=shape), Dense(128, activation=relu), Dense(1)])\n",
    "    model.compile(loss='mse', metrics=['mse'], optimizer=RMSPropOptimizer(learning_rate))\n",
    "    print('Training model: {0}...'.format('lr'))\n",
    "    model.fit(X_train, y_train, epochs=300, verbose=0)\n",
    "\n",
    "    test_predictions = [i[0] for i in model.predict(X_test)]\n",
    "    print('Mean Squared Error (Training):', mean_squared_error(y_test, test_predictions))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Training): 18.902572218543433\n",
      "Wall time: 23min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_nn(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Outperforms Linear Regression model with only a small fraction of the data\n",
    "* Takes significantly longer than other models to run\n",
    "* Scaling improves performance significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 'nn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess/fit model to real test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df = pd.read_csv('./all/test.csv', parse_dates=['pickup_datetime'])\n",
    "preprocess(real_df, training=False)\n",
    "\n",
    "real_X = real_df[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'year', 'month', 'day', 'hour', 'manhattan_distance', 'euclidean_distance', 'passenger_count', 'gas']]\n",
    "\n",
    "if scaled:\n",
    "    real_X = StandardScaler().fit_transform(real_X)\n",
    "\n",
    "if selected_model == 'lr':\n",
    "    model = train_lr(df)\n",
    "    predictions = model.predict(real_X)\n",
    "elif selected_model == 'xgb':\n",
    "    model = train_xgb(df)\n",
    "    predictions = model.predict(xgb.DMatrix(real_X))\n",
    "elif selected_model == 'nn':\n",
    "    model = train_nn(df)\n",
    "    predictions = [i[0] for i in model.predict(real_X)]\n",
    "\n",
    "data = list(zip(real_df['key'], predictions))\n",
    "\n",
    "submission = pd.DataFrame(data, columns=['key', 'fare_amount'])\n",
    "submission.set_index('key', inplace=True)\n",
    "submission.to_csv('submission.csv')\n",
    "print('----------------------------')\n",
    "print('Predictions written to submissions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Scores by model (RMSE):\n",
    "* Linear Regression: 5.64822\n",
    "* Neural Network: 4.36043\n",
    "* Decision Tree Ensembles (XGBoost): 3.51330    <---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
